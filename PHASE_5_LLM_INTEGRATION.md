# FAZ 5 â€“ GERÃ‡EK LLM ENTEGRASYONU

**AmaÃ§:** Stub agent'larÄ± gerÃ§ek Ollama LLM Ã§aÄŸrÄ±larÄ±yla deÄŸiÅŸtirmek, oyunu tam dinamik hale getirmek.

**SÃ¼re Tahmini:** 4-6 saat  
**Ã–nkoÅŸul:** Ollama kurulu ve Ã§alÄ±ÅŸÄ±r durumda (`ollama serve`)

---

## Task 5.1 â€“ Director Agent LLM Entegrasyonu

**Goal:** DirectorAgent'Ä±n scene intent ve player options Ã¼retmesini LLM'e yaptÄ±rmak.

**AdÄ±mlar:**

1. **Prompt Template OluÅŸtur:**
   - `fortress_director/prompts/director_prompt.txt` dosyasÄ±nÄ± dÃ¼zenle/oluÅŸtur
   - Projected state, metrics, recent events kullan
   - Few-shot examples ekle (2-3 Ã¶rnek scene intent)
   - Output format: JSON (scene_intent + player_options)

2. **OllamaClient Entegre Et:**
   ```python
   # fortress_director/agents/director_agent.py
   from fortress_director.llm.ollama_client import OllamaClient
   
   class DirectorAgent:
       def __init__(self, llm_client=None, model="mistral"):
           self._llm_client = llm_client or OllamaClient()
           self._model = model
   
       def generate_intent(self, projected_state, player_choice):
           prompt = self._build_prompt(projected_state, player_choice)
           response = self._llm_client.generate(
               model=self._model,
               prompt=prompt,
               response_format="json"
           )
           return self._parse_output(response)
   ```

3. **Output Parser Yaz:**
   - JSON parse ve validation
   - Schema: `{"scene_intent": {...}, "player_options": [...]}`
   - Fallback: Parse baÅŸarÄ±sÄ±z olursa stub output dÃ¶n

**Tamamlanma Kriteri:**
- `tests/agents/test_director_llm.py` yaz
- Mock Ollama response ile test et
- GerÃ§ek Ollama ile manuel test: `pytest tests/agents/test_director_llm.py -v`

---

## Task 5.2 â€“ Planner Agent LLM Entegrasyonu

**Goal:** PlannerAgent'Ä±n safe function Ã§aÄŸrÄ±larÄ±nÄ± LLM'den almasÄ±.

**AdÄ±mlar:**

1. **Mevcut Prompt'u GÃ¼ncelle:**
   - `planner_agent.py` iÃ§indeki `build_prompt()` zaten hazÄ±r
   - Cluster manager ile relevant functions filtreleniyor âœ…
   - Few-shot examples mevcut âœ…

2. **LLM Ã‡aÄŸrÄ±sÄ±nÄ± Ekle:**
   ```python
   def plan_actions(self, projected_state, scene_intent):
       prompt = self.build_prompt(projected_state, scene_intent)
       
       # LLM'e gÃ¶nder
       response = self._llm_client.generate(
           model=self._model,  # "mistral" veya "qwen"
           prompt=prompt,
           response_format="json",
           options={"temperature": 0.7, "top_p": 0.9}
       )
       
       # Parse ve validate
       try:
           validated = self.validate_llm_output(response["response"])
           actions = [{"function": c["name"], "args": c["kwargs"]} 
                      for c in validated["calls"]]
       except Exception:
           # Fallback to deterministic plan
           validated = self._build_fallback_plan(projected_state, scene_intent)
           actions = ...
       
       return {"prompt": prompt, "raw_plan": validated, "planned_actions": actions}
   ```

3. **Error Handling Ekle:**
   - LLM timeout â†’ fallback plan
   - Invalid JSON â†’ fallback plan
   - Schema validation fail â†’ fallback plan
   - Log all failures for debugging

**Tamamlanma Kriteri:**
- `tests/agents/test_planner_llm.py` yaz
- Mock LLM output ile valid/invalid case'leri test et
- GerÃ§ek Ollama ile 5 turn test: her turn'de LLM'den plan gelmeli

---

## Task 5.3 â€“ WorldRenderer Agent LLM Entegrasyonu

**Goal:** Narrative text ve NPC dialoglarÄ±nÄ± LLM'den Ã¼retmek.

**AdÄ±mlar:**

1. **Prompt Template OluÅŸtur:**
   - `fortress_director/prompts/world_renderer_prompt.txt` oluÅŸtur
   - Input: executed_actions, state_delta, atmosphere cues
   - Output: JSON `{"narrative_block": str, "npc_dialogues": [...], "atmosphere": {...}}`
   - Theme-specific flavor (siege_default iÃ§in)

2. **WorldRendererAgent'Ä± GÃ¼ncelle:**
   ```python
   # fortress_director/agents/world_renderer_agent.py
   class WorldRendererAgent:
       def __init__(self, llm_client=None, model="phi-3"):
           self._llm_client = llm_client or OllamaClient()
           self._model = model
   
       def render(self, world_state, executed_actions):
           prompt = self._build_prompt(world_state, executed_actions)
           response = self._llm_client.generate(
               model=self._model,
               prompt=prompt,
               response_format="json"
           )
           return self._parse_output(response)
   ```

3. **Fallback Template Sistemi:**
   - LLM baÅŸarÄ±sÄ±z olursa ÅŸu anki template-based output kullan
   - Deterministic + LLM hybrid yaklaÅŸÄ±m

**Tamamlanma Kriteri:**
- `tests/agents/test_world_renderer_llm.py` yaz
- Mock output ile test
- GerÃ§ek Ollama ile narrative kalitesini kontrol et

---

## Task 5.4 â€“ LLM Configuration & Model Management

**Goal:** FarklÄ± modelleri kolayca deÄŸiÅŸtirebilmek.

**AdÄ±mlar:**

1. **Config DosyasÄ±nÄ± GÃ¼ncelle:**
   ```yaml
   # fortress_director/config/settings.yaml
   llm:
     ollama:
       base_url: "http://localhost:11434/"
       timeout: 240.0
     models:
       director: "mistral:7b"
       planner: "qwen:1.5b"
       world_renderer: "phi-3:mini"
     options:
       temperature: 0.7
       top_p: 0.9
       top_k: 40
   ```

2. **Model Registry OluÅŸtur:**
   ```python
   # fortress_director/llm/model_registry.py
   from dataclasses import dataclass
   
   @dataclass
   class ModelConfig:
       name: str
       temperature: float = 0.7
       top_p: float = 0.9
   
   DEFAULT_MODELS = {
       "director": ModelConfig("mistral:7b", temperature=0.8),
       "planner": ModelConfig("qwen:1.5b", temperature=0.6),
       "world_renderer": ModelConfig("phi-3:mini", temperature=0.9),
   }
   ```

3. **Health Check Tool:**
   ```python
   # scripts/dev_tools.py'ye ekle
   def check_llm_health():
       """Verify Ollama is running and models are available."""
       client = OllamaClient()
       for agent, model in DEFAULT_MODELS.items():
           try:
               client.generate(model=model.name, prompt="test", options={"max_tokens": 1})
               print(f"âœ… {agent}: {model.name} OK")
           except Exception as e:
               print(f"âŒ {agent}: {model.name} FAIL - {e}")
   ```

**Tamamlanma Kriteri:**
- `py scripts/dev_tools.py check_llm` komutu Ã§alÄ±ÅŸÄ±yor
- Agent'lar config'den model okuyabiliyor
- Model deÄŸiÅŸtirme settings.yaml'dan yapÄ±labiliyor

---

## Task 5.5 â€“ End-to-End LLM Test

**Goal:** TÃ¼m pipeline'Ä± gerÃ§ek LLM ile test etmek.

**AdÄ±mlar:**

1. **Integration Test Yaz:**
   ```python
   # tests/integration/test_llm_pipeline.py
   import pytest
   from fortress_director.pipeline.turn_manager import run_turn
   
   @pytest.mark.integration
   def test_full_turn_with_llm():
       """Run one turn with real LLM calls."""
       result = run_turn(player_choice="option_1")
       assert result["narrative"]
       assert len(result["executed_actions"]) > 0
       assert result["player_options"]
   
   @pytest.mark.integration
   def test_golden_path_with_llm():
       """Run 7-turn demo scenario with LLM."""
       for turn in range(7):
           result = run_turn(player_choice=f"option_{(turn % 3) + 1}")
           assert "stability" in result.get("hud", {})
   ```

2. **Performance Benchmark:**
   ```python
   def benchmark_llm_latency():
       """Measure average turn time with LLM."""
       import time
       times = []
       for _ in range(10):
           start = time.time()
           run_turn(player_choice="option_1")
           times.append(time.time() - start)
       print(f"Avg turn time: {sum(times)/len(times):.2f}s")
   ```

3. **Quality Assurance:**
   - 10 turn Ã§alÄ±ÅŸtÄ±r, narrative'leri oku
   - TutarlÄ±lÄ±k kontrolÃ¼ (NPC isimleri, location'lar)
   - Function Ã§aÄŸrÄ±larÄ± mantÄ±klÄ± mÄ±?

**Tamamlanma Kriteri:**
- Integration testler geÃ§iyor
- Ortalama turn sÃ¼resi < 10 saniye
- Narrative kalitesi kabul edilebilir

---

## Task 5.6 â€“ UI'da LLM Status GÃ¶sterimi

**Goal:** KullanÄ±cÄ± LLM Ã§alÄ±ÅŸÄ±p Ã§alÄ±ÅŸmadÄ±ÄŸÄ±nÄ± gÃ¶rebilmeli.

**AdÄ±mlar:**

1. **Backend Status Endpoint:**
   ```python
   # fortress_director/api.py
   @app.get("/api/status")
   def get_status():
       """Return system health including LLM availability."""
       llm_health = {}
       for agent, model in DEFAULT_MODELS.items():
           try:
               client.generate(model=model.name, prompt="ping", options={"max_tokens": 1})
               llm_health[agent] = "online"
           except:
               llm_health[agent] = "offline"
       return {"llm": llm_health, "version": API_VERSION}
   ```

2. **UI Status Indicator:**
   - HUD'da veya Debug Panel'de LLM status gÃ¶ster
   - ğŸŸ¢ Online / ğŸ”´ Offline / ğŸŸ¡ Fallback Mode

3. **Fallback Mode Toggle:**
   - Settings'ten "Use Stub Agents" toggle ekle
   - Demo iÃ§in LLM kapalÄ± gÃ¶sterebilme

**Tamamlanma Kriteri:**
- UI'da LLM status gÃ¶rÃ¼nÃ¼yor
- Ollama kapatÄ±nca "offline" gÃ¶steriyor
- Fallback mode Ã§alÄ±ÅŸÄ±yor

---

## ğŸ“Š Faz 5 BaÅŸarÄ± Kriterleri

âœ… **Director Agent:**
- LLM'den scene intent ve player options alÄ±yor
- Parse hatasÄ± olunca fallback Ã§alÄ±ÅŸÄ±yor
- Test coverage > %80

âœ… **Planner Agent:**
- LLM'den safe function planÄ± alÄ±yor
- Cluster manager ile prompt optimize
- Validation + fallback mekanizmasÄ±

âœ… **WorldRenderer Agent:**
- LLM'den narrative Ã¼retiyor
- NPC dialoglarÄ± dinamik
- Atmosphere cues Ã§alÄ±ÅŸÄ±yor

âœ… **Configuration:**
- Model seÃ§imi config'den yapÄ±labiliyor
- LLM health check Ã§alÄ±ÅŸÄ±yor
- Performance kabul edilebilir (<10s/turn)

âœ… **Integration:**
- 7-turn Golden Path LLM ile Ã§alÄ±ÅŸÄ±yor
- Smoke test 20 run geÃ§iyor
- UI'da LLM status gÃ¶rÃ¼nÃ¼yor

---

## ğŸš€ Deployment Checklist

- [ ] Ollama kurulum dokÃ¼mantasyonu (`docs/llm_setup.md`)
- [ ] Model indirme scripti (`scripts/download_models.sh`)
- [ ] Environment variable setup (`.env.example`)
- [ ] LLM olmadan da Ã§alÄ±ÅŸma modu (offline/demo mode)
- [ ] Error logging ve monitoring

---

## ğŸ“ Notlar

**Model Ã–nerileri:**
- **Director**: Mistral 7B (reasoning + options generation)
- **Planner**: Qwen 1.5B (hÄ±zlÄ±, function calling iÃ§in yeterli)
- **WorldRenderer**: Phi-3 Mini (creative narrative iÃ§in iyi)

**Alternatif:**
- Hepsi iÃ§in Llama 3.1 8B (tek model, tutarlÄ±lÄ±k)
- GPT-4 API (cloud, daha kaliteli ama maliyet)

**Performance Tips:**
- Planner iÃ§in caching (aynÄ± context â†’ aynÄ± plan)
- Parallel LLM calls (3 agent async olabilir)
- Prompt optimization (token sayÄ±sÄ±nÄ± azalt)
