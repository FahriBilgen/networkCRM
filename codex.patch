 (cd "$(git rev-parse --show-toplevel)" && git apply --3way <<'EOF' 
diff --git a/agents/base_agent.py b/agents/base_agent.py
index f2020e9eb2035cceeb26be8aa40f99d571e05d37..24929ca15511fb143fc2cb84c7bbffc276431b7d 100644
--- a/agents/base_agent.py
+++ b/agents/base_agent.py
@@ -1,42 +1,41 @@
 """Common agent helpers for Fortress Director."""
 
 from __future__ import annotations
 
 import json
 import os
 from dataclasses import dataclass, field
 from pathlib import Path
 from typing import Any, Dict, Optional
 
 from llm.ollama_client import (
     OllamaClient,
     OllamaClientConfig,
     OllamaClientError,
 )
 from settings import ModelConfig, SETTINGS
-from llm.offline_client import OfflineOllamaClient
 
 
 class AgentError(RuntimeError):
     """Base class for agent-related failures."""
 
 
 class AgentOutputError(AgentError):
     """Raised when an agent returns invalid JSON."""
 
 
 @dataclass
 class PromptTemplate:
     """Small helper to load and render prompt templates."""
 
     path: Path
     cache: bool = True
     _cached_value: Optional[str] = field(default=None, init=False, repr=False)
 
     def load(self) -> str:
         if self.cache and self._cached_value is not None:
             return self._cached_value
         value = self.path.read_text(encoding="utf-8")
         if self.cache:
             self._cached_value = value
         return value
@@ -110,44 +109,50 @@ class BaseAgent:
             "top_p": self._model.top_p,
             "num_predict": self._model.max_tokens,
         }
         if options_override:
             options.update(options_override)
         return options
 
     def _parse_json(self, text: str) -> Any:
         if not text:
             raise AgentOutputError(f"{self.name} agent returned empty output")
         try:
             return json.loads(text)
         except json.JSONDecodeError as exc:
             raise AgentOutputError(
                 f"{self.name} agent produced invalid JSON: {text}"
             ) from exc
 
 
 def build_prompt_path(filename: str) -> Path:
     """Resolve a prompt file relative to the project root."""
 
     return SETTINGS.project_root / "prompts" / filename
 
 
 def default_ollama_client(agent_key: Optional[str] = None) -> OllamaClient:
-    """Create an Ollama-compatible client with an offline fallback."""
+    """Create an Ollama client that always talks to a live model service."""
 
-    if os.environ.get("FORTRESS_USE_OLLAMA") == "1":
-        config = OllamaClientConfig(
-            base_url=SETTINGS.ollama_base_url,
-            timeout=SETTINGS.ollama_timeout,
-        )
-        return OllamaClient(config)
-
-    return OfflineOllamaClient(agent_key or "generic")
+    base_url = os.environ.get("FORTRESS_OLLAMA_BASE_URL", SETTINGS.ollama_base_url)
+    timeout_value = os.environ.get("FORTRESS_OLLAMA_TIMEOUT")
+    try:
+        timeout = float(timeout_value) if timeout_value is not None else SETTINGS.ollama_timeout
+    except ValueError as exc:  # pragma: no cover - defensive guard
+        raise AgentError(
+            "FORTRESS_OLLAMA_TIMEOUT must be a float value"
+        ) from exc
+
+    config = OllamaClientConfig(
+        base_url=base_url,
+        timeout=timeout,
+    )
+    return OllamaClient(config)
 
 
 def get_model_config(agent_key: str) -> ModelConfig:
     """Fetch model configuration for the requested agent type."""
 
     try:
         return SETTINGS.models[agent_key]
     except KeyError as exc:  # pragma: no cover - defensive guard
         raise AgentError(f"Unknown agent key: {agent_key}") from exc
diff --git a/fortress_director/agents/base_agent.py b/fortress_director/agents/base_agent.py
index afa20ca1a4e6329c5b24aa8e2d5f3f2b51947ae2..f4c8447b70b13e31d738d76e6cec67f403e646bd 100644
--- a/fortress_director/agents/base_agent.py
+++ b/fortress_director/agents/base_agent.py
@@ -1,42 +1,41 @@
 """Common agent helpers for Fortress Director."""
 
 from __future__ import annotations
 
 import json
 import os
 from dataclasses import dataclass, field
 from pathlib import Path
 from typing import Any, Dict, Optional
 
 from fortress_director.llm.ollama_client import (
     OllamaClient,
     OllamaClientConfig,
     OllamaClientError,
 )
 from fortress_director.settings import ModelConfig, SETTINGS
-from fortress_director.llm.offline_client import OfflineOllamaClient
 
 
 class AgentError(RuntimeError):
     """Base class for agent-related failures."""
 
 
 class AgentOutputError(AgentError):
     """Raised when an agent returns invalid JSON."""
 
 
 @dataclass
 class PromptTemplate:
     """Small helper to load and render prompt templates."""
 
     path: Path
     cache: bool = True
     _cached_value: Optional[str] = field(default=None, init=False, repr=False)
 
     def load(self) -> str:
         if self.cache and self._cached_value is not None:
             return self._cached_value
         value = self.path.read_text(encoding="utf-8")
         if self.cache:
             self._cached_value = value
         return value
@@ -114,44 +113,50 @@ class BaseAgent:
             "top_p": self._model.top_p,
             "num_predict": self._model.max_tokens,
         }
         if options_override:
             options.update(options_override)
         return options
 
     def _parse_json(self, text: str) -> Any:
         if not text:
             raise AgentOutputError(f"{self.name} agent returned empty output")
         try:
             return json.loads(text)
         except json.JSONDecodeError as exc:
             raise AgentOutputError(
                 f"{self.name} agent produced invalid JSON: {text}"
             ) from exc
 
 
 def build_prompt_path(filename: str) -> Path:
     """Resolve a prompt file relative to the project root."""
 
     return SETTINGS.project_root / "prompts" / filename
 
 
 def default_ollama_client(agent_key: Optional[str] = None) -> OllamaClient:
-    """Create an Ollama-compatible client with an offline fallback."""
+    """Create an Ollama client that always talks to a live model service."""
 
-    if os.environ.get("FORTRESS_USE_OLLAMA") == "1":
-        config = OllamaClientConfig(
-            base_url=SETTINGS.ollama_base_url,
-            timeout=SETTINGS.ollama_timeout,
-        )
-        return OllamaClient(config)
-
-    return OfflineOllamaClient(agent_key or "generic")
+    base_url = os.environ.get("FORTRESS_OLLAMA_BASE_URL", SETTINGS.ollama_base_url)
+    timeout_value = os.environ.get("FORTRESS_OLLAMA_TIMEOUT")
+    try:
+        timeout = float(timeout_value) if timeout_value is not None else SETTINGS.ollama_timeout
+    except ValueError as exc:  # pragma: no cover - defensive guard
+        raise AgentError(
+            "FORTRESS_OLLAMA_TIMEOUT must be a float value"
+        ) from exc
+
+    config = OllamaClientConfig(
+        base_url=base_url,
+        timeout=timeout,
+    )
+    return OllamaClient(config)
 
 
 def get_model_config(agent_key: str) -> ModelConfig:
     """Fetch model configuration for the requested agent type."""
 
     try:
         return SETTINGS.models[agent_key]
     except KeyError as exc:  # pragma: no cover - defensive guard
         raise AgentError(f"Unknown agent key: {agent_key}") from exc
diff --git a/pytest.ini b/pytest.ini
index 9b944bdc0b590396d8e12af66c327904884f88d2..fd86346d7ca7170622f99cf9f380a09aa58dbe37 100644
--- a/pytest.ini
+++ b/pytest.ini
@@ -1,3 +1,4 @@
 [pytest]
 testpaths = tests
 log_cli = true
+pythonpath = .
 
EOF
)
